import os
import random
from abc import ABC, abstractmethod
import math

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List

# ---------------------------------
# --- Global Parameters (èˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´) ---
# ---------------------------------
HIST_LEN = 30
EMBED_DIM = 256
NHEAD = 4
NUM_LAYERS = 4
MLP_HIDDEN = 256
OBS_DIM = 5
ACTION_DIM = 3

# --- æ¨¡å‹å’Œæ•¸æ“šè·¯å¾‘ ---
MODEL_DIR = os.path.join(os.path.dirname(__file__), "checkpoints", "v1.2")  # å‡è¨­æ¨¡å‹æ–‡ä»¶èˆ‡æ­¤è…³æœ¬åœ¨åŒä¸€ç›®éŒ„ä¸‹
IQL_ACTOR_PATH = os.path.join(MODEL_DIR, "industrial_iql_actor.pth")
IQL_EMBED_PATH = os.path.join(MODEL_DIR, "industrial_iql_embed.pth")
NORMALIZER_MEAN_PATH = os.path.join(MODEL_DIR, "normalizer_mean.npy")
NORMALIZER_STD_PATH = os.path.join(MODEL_DIR, "normalizer_std.npy")


# -----------------------------------------------------------
# --- 1. å¾è¨“ç·´ä»£ç¢¼ä¸­è¤‡è£½å¿…è¦çš„ç¶²çµ¡å®šç¾© ---
# -----------------------------------------------------------

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 50):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        pe = pe.permute(1, 0, 2)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class SequenceEmbedding(nn.Module):
    def __init__(self, obs_dim, hist_len, embed_dim, nhead, num_layers):
        super().__init__()
        self.input_embed = nn.Linear(obs_dim, embed_dim)
        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hist_len)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=nhead, dim_feedforward=MLP_HIDDEN,
            dropout=0.1, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, obs_seq: torch.Tensor) -> torch.Tensor:
        x = self.input_embed(obs_seq)
        x = self.pos_encoder(x)
        transformer_out = self.transformer_encoder(x)
        return transformer_out[:, -1, :]


def build_mlp(input_dim, output_dim, hidden_units=[256, 256]):
    layers = [nn.Linear(input_dim, hidden_units[0]), nn.ReLU()]
    for i in range(len(hidden_units) - 1):
        layers.append(nn.Linear(hidden_units[i], hidden_units[i + 1]))
        layers.append(nn.ReLU())
    layers.append(nn.Linear(hidden_units[-1], output_dim))
    return nn.Sequential(*layers)


class Actor(nn.Module):
    def __init__(self, embed_dim, action_dim):
        super().__init__()
        self.net = build_mlp(embed_dim, action_dim)

    def forward(self, embed):
        return torch.tanh(self.net(embed))


# -----------------------------------------------------------
# --- 2. æ‚¨æä¾›çš„ BaseAgent æ¡†æ¶ ---
# -----------------------------------------------------------
class BaseAgent(ABC):
    """
    åŸºç±»ï¼Œå®šä¹‰äº†æ‰€æœ‰ Agent çš„ç»Ÿä¸€æ¥å£ï¼è¡Œä¸ºï¼š
      - éšæœºç§å­ç®¡ç†
      - è®¾å¤‡ï¼ˆCPU/CUDAï¼‰é€‰æ‹©
      - è§‚æµ‹ä¸åŠ¨ä½œçš„å†å²ç¼“å­˜
      - åŠ¨ä½œäº§ç”Ÿçš„ç»Ÿä¸€æµç¨‹ï¼ˆreshape â†’ å‰å‘æ¨ç† â†’ clip â†’ ç¼“å­˜ â†’ è¿”å›ï¼‰
    """

    def __init__(self, seed: int = None):
        if seed is not None:
            self.seed(seed)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.obs_history = []
        self.act_history = []

    def seed(self, seed: int = 123) -> None:
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)

    def reset(self) -> None:
        self.obs_history.clear()
        self.act_history.clear()

    def act(self, obs: np.ndarray) -> np.ndarray:
        obs = obs.reshape(-1).astype(np.float32)
        action = self.get_action(obs)
        action = np.clip(action, -1.0, 1.0).reshape(-1).astype(np.float32)
        self.obs_history.append(obs)
        self.act_history.append(action)
        return action

    @abstractmethod
    def get_action(self, obs: np.ndarray) -> np.ndarray:
        ...

    def close(self) -> None:
        pass


# -----------------------------------------------------------
# --- 3. å¯¦ç¾ PolicyAgent (åŸ IQLAgent) ---
# -----------------------------------------------------------
# ã€é—œéµä¿®æ”¹ã€‘: å°‡é¡åˆ¥åç¨±å¾ IQLAgent æ›´æ”¹ç‚º PolicyAgent ä»¥åŒ¹é… evaluator.py
class PolicyAgent(BaseAgent):
    """
    åŸºæ–¼é è¨“ç·´çš„ History-Aware IQL æ¨¡å‹çš„æ™ºèƒ½é«”ã€‚
    - åŠ è¼‰åºåˆ—åµŒå…¥ç¶²çµ¡å’Œ Actor ç¶²çµ¡ã€‚
    - åŠ è¼‰è¨“ç·´æ™‚è¨ˆç®—çš„å‡å€¼å’Œæ¨™æº–å·®ï¼Œç”¨æ–¼æ­¸ä¸€åŒ–è¼¸å…¥ã€‚
    - ç¶­è­·æœ€è¿‘ 30 å¹€çš„è§€æ¸¬æ­·å²ï¼Œä¸¦åœ¨æ¨ç†æ™‚ä½¿ç”¨ã€‚
    """

    def __init__(self):
        super().__init__()
        print(f"Initializing PolicyAgent (IQL-based) on device: {self.device}")

        # 1. åŠ è¼‰ Normalizer çš„çµ±è¨ˆæ•¸æ“š
        try:
            self.obs_mean = np.load(NORMALIZER_MEAN_PATH)
            self.obs_std = np.load(NORMALIZER_STD_PATH)
            print("âœ… Successfully loaded normalizer mean and std.")
            print(f"   - Mean shape: {self.obs_mean.shape}")
            print(f"   - Std shape: {self.obs_std.shape}")
        except FileNotFoundError as e:
            print(f"âŒ Error: Normalizer file not found. Make sure to save them during training. {e}")
            raise

        # 2. åˆå§‹åŒ–ç¶²çµ¡çµæ§‹
        self.embed_net = SequenceEmbedding(OBS_DIM, HIST_LEN, EMBED_DIM, NHEAD, NUM_LAYERS)
        self.actor = Actor(EMBED_DIM, ACTION_DIM)

        # 3. åŠ è¼‰é è¨“ç·´æ¬Šé‡
        try:
            load_kwargs = {'map_location': self.device}
            self.embed_net.load_state_dict(torch.load(IQL_EMBED_PATH, weights_only=True, **load_kwargs))
            self.actor.load_state_dict(torch.load(IQL_ACTOR_PATH, weights_only=True, **load_kwargs))
            print("âœ… Successfully loaded pretrained embedding and actor models.")
        except FileNotFoundError as e:
            print(f"âŒ Error: Model file not found. Check paths: {IQL_EMBED_PATH} or {IQL_ACTOR_PATH}. {e}")
            raise

        # 4. å°‡ç¶²çµ¡ç§»å‹•åˆ°æŒ‡å®šè¨­å‚™ä¸¦åˆ‡æ›åˆ°è©•ä¼°æ¨¡å¼
        self.embed_net.to(self.device)
        self.actor.to(self.device)
        self.embed_net.eval()
        self.actor.eval()
        print("ğŸš€ Agent is ready.")

    def _normalize_obs(self, obs: np.ndarray) -> np.ndarray:
        """ä½¿ç”¨åŠ è¼‰çš„å‡å€¼å’Œæ¨™æº–å·®ä¾†æ­¸ä¸€åŒ–è§€æ¸¬å€¼ã€‚"""
        # å¢åŠ ä¸€å€‹å°çš„ epsilon é˜²æ­¢é™¤ä»¥é›¶
        return (obs - self.obs_mean) / (self.obs_std + 1e-8)

    def get_action(self, obs: np.ndarray) -> np.ndarray:
        """
        æ ¹æ“šç•¶å‰è§€æ¸¬å’Œæ­·å²ç·©å­˜æ§‹é€ ç¶²çµ¡è¼¸å…¥ä¸¦é€²è¡Œå‰å‘æ¨ç†ã€‚
        """
        # 1. æ§‹å»ºåŒ…å«ç•¶å‰è§€æ¸¬çš„æ­·å²åºåˆ— (ä½¿ç”¨æœªæ­¸ä¸€åŒ–çš„åŸå§‹è§€æ¸¬)
        # BaseAgent çš„ obs_history å·²ç¶“å­˜å„²äº†éå»çš„è§€æ¸¬
        seq_unnormalized = self.obs_history + [obs]

        # 2. å¡«å……æˆ–æˆªæ–·æ­·å²åºåˆ—ä»¥æ»¿è¶³ HIST_LEN
        if len(seq_unnormalized) < HIST_LEN:
            # ç•¶æ­·å²ä¸è¶³æ™‚ï¼Œä½¿ç”¨æœ€æ—©çš„ä¸€å¹€é€²è¡Œå¡«å……
            padding = [seq_unnormalized[0]] * (HIST_LEN - len(seq_unnormalized))
            obs_seq_unnormalized = np.array(padding + seq_unnormalized, dtype=np.float32)
        else:
            # ç•¶æ­·å²å……è¶³æ™‚ï¼Œå–æœ€æ–°çš„ HIST_LEN å¹€
            obs_seq_unnormalized = np.array(seq_unnormalized[-HIST_LEN:], dtype=np.float32)

        # 3. ã€é—œéµã€‘å°æ•´å€‹åºåˆ—é€²è¡Œæ­¸ä¸€åŒ–
        obs_seq_normalized = self._normalize_obs(obs_seq_unnormalized)

        # 4. è½‰æ›çˆ² Tensorï¼Œæ·»åŠ  batch ç¶­åº¦ä¸¦ç§»å‹•åˆ° device
        obs_tensor = torch.from_numpy(obs_seq_normalized).unsqueeze(0).to(self.device)

        # 5. ç„¡æ¢¯åº¦å‰å‘æ¨ç†
        with torch.no_grad():
            # (1, HIST_LEN, OBS_DIM) -> (1, EMBED_DIM)
            state_embedding = self.embed_net(obs_tensor)
            # (1, EMBED_DIM) -> (1, ACTION_DIM)
            action_tensor = self.actor(state_embedding)

        # 6. å»é™¤ batch ç¶­åº¦ï¼Œè½‰å› Numpy æ•¸çµ„ä¸¦è¿”å›
        return action_tensor.squeeze(0).cpu().numpy()


# -----------------------------------------------------------
# --- 4. ä¸»ç¨‹åºå…¥å£ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ PolicyAgent ---
# -----------------------------------------------------------
if __name__ == "__main__":
    print("--- Testing PolicyAgent (IQL-based) ---")

    # åœ¨é‹è¡Œæ­¤è…³æœ¬å‰ï¼Œè«‹ç¢ºä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨æ–¼åŒç´šç›®éŒ„ä¸‹ï¼š
    # - industrial_iql_actor.pth
    # - industrial_iql_embed.pth
    # - normalizer_mean.npy
    # - normalizer_std.npy
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯ä»¥å…ˆå‰µå»ºåƒé€ çš„ä½”ä½æ–‡ä»¶ä¾†æ¸¬è©¦ä»£ç¢¼çµæ§‹ã€‚

    try:
        # å¯¦ä¾‹åŒ–æ™ºèƒ½é«”
        agent = PolicyAgent() # <-- åç¨±å·²æ›´æ–°

        # é‡ç½®æ™ºèƒ½é«”ç‹€æ…‹ (æ¸…ç©ºæ­·å²è¨˜éŒ„)
        agent.reset()

        # æ¨¡æ“¬ä¸€å€‹ episodeï¼Œå…± 50 æ­¥
        print("\n--- Simulating one episode (50 steps) ---")
        for i in range(50):
            # å‰µå»ºä¸€å€‹ç¬¦åˆç¶­åº¦çš„éš¨æ©Ÿè§€æ¸¬
            # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™å°‡ä¾†è‡ªæ–¼æ‚¨çš„ç’°å¢ƒ env.step()
            dummy_obs = np.random.rand(OBS_DIM).astype(np.float32)

            # è®“æ™ºèƒ½é«”æ ¹æ“šè§€æ¸¬æ¡å–è¡Œå‹•
            action = agent.act(dummy_obs)

            print(f"Step {i + 1:02d} | Obs shape: {dummy_obs.shape} -> Action: {action}")
            # æ‰“å°æ­·å²é•·åº¦ï¼Œä»¥é©—è­‰å…¶å¢é•·å’Œæˆªæ–·
            if (i + 1) % 10 == 0:
                print(f"         (Current obs_history length: {len(agent.obs_history)})")

        print("\n--- Simulation finished ---")

        # æ¸…ç†è³‡æº
        agent.close()

    except (FileNotFoundError, RuntimeError) as e:
        print(f"\nCould not run simulation due to an error: {e}")
        print(
            "Please make sure all required model and normalizer files are present in the same directory as this script.")