# -*- coding: utf-8 -*-
"""MOPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lrjb3JzHmtpFEL5m-Jg19QUwZW4Pz61J
"""

import neorl2
import gymnasium as gym

print("\n--- 验证环境 ---")
try:
    # 尝试创建 Pipeline 环境
    env = gym.make("Pipeline")
    print(f"✅ NeoRL-2 环境 'Pipeline' 成功创建！Obs Dim: {env.observation_space.shape}, Act Dim: {env.action_space.shape}")

    # 尝试获取数据集 (数据已在本地，不会重新下载)
    train_data, val_data = env.get_dataset()

    # !!! 修正：使用实际的键名 'obs' 替换 'observations' !!!
    train_obs_count = len(train_data['obs'])
    print(f"✅ 成功获取数据集。训练集样本数: {train_obs_count}")

    env.close()

    print("\n===============================")
    print("NeoRL-2 环境和数据集现已完全准备就绪！")
    print("===============================")

except Exception as e:
    print(f"❌ 最终错误：{e}")

import torch
import numpy as np
# import pandas as pd # [修改] 移除 pandas
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, random_split, DataLoader
import os # [新增] 导入 os 用于创建目录

# --- [新增] 导入 neorl2 和 gymnasium ---
import neorl2
import gymnasium as gym

# -----------------------\n
# 定义三层 MLP
# -----------------------
class Mlp(nn.Module):
    def __init__(self,
      # [修改] 适配 "Pipeline" Obs Dim (52)
      input_dim: int = 52,
      hidden_dim1: int = 256,
      hidden_dim2: int = 128,
      # [修改] 适配 "Pipeline" Act Dim (1)
      output_dim: int = 1):
        super().__init__()
        self.fc1 = nn.Linear(input_dim,  hidden_dim1)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.fc3 = nn.Linear(hidden_dim2, output_dim)
        print(f"[Mlp] Model initialized: Input={input_dim}, Output={output_dim}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# -----------------------\n
# 训练和验证函数 (保持不变)
# -----------------------
def train_one_epoch(model, optimizer, loader, device):
    model.train()
    total_loss = 0.0
    for obs, act in loader:
        obs = obs.to(device)
        act = act.to(device)

        pred = model(obs)
        loss = F.mse_loss(pred, act)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * obs.size(0)
    return total_loss / len(loader.dataset)

@torch.no_grad()
def validate(model, loader, device):
    model.eval()
    total_loss = 0.0
    for obs, act in loader:
        obs = obs.to(device)
        act = act.to(device)
        pred = model(obs)
        loss = F.mse_loss(pred, act)
        total_loss += loss.item() * obs.size(0)
    return total_loss / len(loader.dataset)

# --- [新增] 在线评估函数 ---
@torch.no_grad()
def evaluate_policy_online(model, device, eval_episodes=10):
    """
    使用 Neorl2 "Pipeline" 环境在线评估当前策略。
    """
    print(f"\n--- 启动在线策略评估 (运行 {eval_episodes} 轮) ---")
    try:
        eval_env = gym.make("Pipeline")
    except Exception as e:
        print(f"❌ 评估错误: 无法创建 'Pipeline' 环境: {e}")
        return

    model.eval() # 确保是评估模式
    total_rewards = []

    for i in range(eval_episodes):
        obs, info = eval_env.reset()
        episode_reward = 0
        terminated = False
        truncated = False

        while not (terminated or truncated):
            # 1. 准备 state tensor (BC 不使用 Normalizer, 直接输入)
            obs_tensor = torch.FloatTensor(obs).to(device)
            if obs_tensor.dim() == 1:
                obs_tensor = obs_tensor.unsqueeze(0) # (52,) -> (1, 52)

            # 2. 获取 action
            action_tensor = model(obs_tensor)
            action = action_tensor.detach().cpu().numpy().flatten() # (1, 1) -> (1,)

            # 3. [重要] BC 模型输出是线性的，但环境需要 [-1, 1]
            # 我们必须裁剪 (clip) 动作以符合环境的动作空间
            action_clipped = np.clip(action, -1.0, 1.0)

            # 4. Step
            next_obs, reward, terminated, truncated, _ = eval_env.step(action_clipped)

            episode_reward += reward
            obs = next_obs

        total_rewards.append(episode_reward)
        print(f"  Eval Episode {i+1}/{eval_episodes}, Reward: {episode_reward:.2f}")

    eval_env.close()

    avg_reward = np.mean(total_rewards)
    std_reward = np.std(total_rewards)
    print("--------------------------------------------------")
    print(f"✅ 在线评估完成: 平均累积奖励: {avg_reward:.2f} +/- {std_reward:.2f} (在 {eval_episodes} 轮中)")
    print("--------------------------------------------------")
    return avg_reward

# -----------------------\n
# 主程序
# -----------------------

# --- [修改] 数据加载 ---
print("--- 正在加载 NeoRL-2 'Pipeline' 数据集 ---")
try:
    env = gym.make("Pipeline")
    # 我们只需要训练集，因为脚本会自己划分验证集
    train_data, _ = env.get_dataset()
    env.close()

    # 标准 BC (s_t, a_t)
    obs = train_data['obs']
    act = train_data['action']

    obs_tensor = torch.from_numpy(obs).float()
    act_tensor = torch.from_numpy(act).float()

    print(f"✅ 成功加载 {len(obs_tensor)} 个 (s, a) 训练对。")
    print(f"Obs shape: {obs_tensor.shape}, Act shape: {act_tensor.shape}")

except Exception as e:
    print(f"❌ 错误: 加载 NeoRL-2 数据集失败: {e}")
    print("请确保 'neorl2' 和 'gymnasium' 已正确安装。")
    # 在主程序中，我们可能希望退出
    exit()

# [修改] 移除 pandas 和历史拼接循环 (从 data = ... 到 action_list.append(...))

# 构建 Dataset
dataset = TensorDataset(obs_tensor, act_tensor)

# 划分训练/验证集：90% 训练，10% 验证 (逻辑保持不变)
val_size   = int(len(dataset) * 0.1)
train_size = len(dataset) - val_size
train_ds, val_ds = random_split(
    dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)
print(f"数据集划分: {train_size} 训练 / {val_size} 验证")

# DataLoader (逻辑保持不变)
batch_size = 256
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)

# 设备和模型 (逻辑保持不变)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Mlp() 将使用我们修改后的新默认值 (input=52, output=1)
model  = Mlp().to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# 用来追踪效果最好的验证损失 (逻辑保持不变)
best_val_loss = float('inf')
best_model_path = "agent/mlp_model.pth"
# [新增] 确保 'agent' 目录存在
os.makedirs(os.path.dirname(best_model_path), exist_ok=True)


# 训练与验证循环 (逻辑保持不变)
epochs = 500
print(f"--- 开始 BC 训练 ({epochs} epochs) ---")
for epoch in range(1, epochs + 1):
    train_loss = train_one_epoch(model, optimizer, train_loader, device)
    val_loss   = validate(model, val_loader, device)

    if epoch % 20 == 0 or epoch == 1: # 每 20 轮打印一次
        print(f"Epoch {epoch:03d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}")

    # 如果当前验证集损失更好，则保存到 CPU 并序列化
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        # 将模型移动到 CPU
        model_cpu = model.to("cpu")
        # 保存 state_dict
        torch.save(model_cpu.state_dict(), best_model_path)
        print(f"  -> Epoch {epoch:03d} New best model saved (val_loss: {best_val_loss:.6f}) to `{best_model_path}`")
        # 再次将模型移动回训练设备
        model = model_cpu.to(device)

print(f"\n--- 训练结束，最优验证损失(MSE): {best_val_loss:.6f} ---")
print(f"最佳模型已保存为 `{best_model_path}`")

# --- [新增] 加载最佳模型并进行在线评估 ---
print("\n--- 加载最佳 BC 模型进行在线评估 ---")
try:
    best_model = Mlp().to(device) # 实例化一个新模型 (使用 52, 1 默认值)

    # [修改] map_location=device 确保模型加载到正确的设备
    best_model.load_state_dict(torch.load(best_model_path, map_location=device))
    print(f"成功从 `{best_model_path}` 加载模型权重。")

    # 调用在线评估函数
    evaluate_policy_online(best_model, device, eval_episodes=10)

except FileNotFoundError:
    print(f"❌ 评估失败: 找不到模型文件 {best_model_path}")
except Exception as e:
    print(f"❌ 评估失败: 加载模型时出错: {e}")

print("\n--- BC 脚本运行完毕 ---")

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal, TransformedDistribution
from torch.distributions.transforms import TanhTransform
import numpy as np
import copy
from typing import List, Tuple, Dict
import os

# --- [新增] 导入 neorl2 和 gymnasium ---
import neorl2
import gymnasium as gym

# --- 全局设置与超参数 ---

# "Pipeline" Env Obs Dim: (52,)
OBS_DIM = 52
HISTORY_LEN = 1
# 状态维度现在等于观测维度
STATE_DIM = OBS_DIM * HISTORY_LEN
# "Pipeline" Env Act Dim: (1,)
ACTION_DIM = 1
# "Pipeline" Env Action Range: [-1.0, 1.0]

# 模型超参数
HIDDEN_DIM = 256
NUM_ENSEMBLE = 7
ROLLOUT_LENGTH = 5
UNCERTAINTY_LAMBDA = 1.0

# 训练超参数
BATCH_SIZE = 256
GAMMA = 0.99
TAU = 0.005
LR_A = 3e-4
LR_C = 3e-4
LR_DYN = 1e-3
ALPHA = 0.2
TARGET_ENTROPY = -float(ACTION_DIM)
POLICY_UPDATE_FREQ = 2
DYNAMICS_TRAIN_EPOCHS = 30000

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")
print(f"[Config] State Dim: {STATE_DIM} (Obs: {OBS_DIM} * Hist: {HISTORY_LEN})")
print(f"[Config] Action Dim: {ACTION_DIM}")


# --- [新增] 数据归一化辅助类 ---
class Normalizer:
    """用于状态归一化的辅助类"""

    def __init__(self, size):
        self.size = size
        self.mean = np.zeros(size)
        self.std = np.ones(size)

    def fit(self, data):
        """根据数据计算均值和标准差"""
        self.mean = np.mean(data, axis=0)
        self.std = np.std(data, axis=0)
        # 防止标准差为0
        self.std[self.std < 1e-12] = 1.0
        print("[Normalizer] Fitted with data. Mean and std calculated.")

    def transform(self, data):
        """对数据进行归一化 (支持 numpy 和 torch tensor)"""
        if isinstance(data, torch.Tensor):
            device = data.device
            mean = torch.FloatTensor(self.mean).to(device)
            std = torch.FloatTensor(self.std).to(device)
            return (data - mean) / std
        return (data - self.mean) / self.std

    def inverse_transform(self, data):
        """对数据进行反归一化"""
        if isinstance(data, torch.Tensor):
            device = data.device
            mean = torch.FloatTensor(self.mean).to(device)
            std = torch.FloatTensor(self.std).to(device)
            return data * std + mean
        return data * self.std + self.mean


# --- 1. 数据加载与回放池 ---

class ReplayBuffer:
    """标准回放池"""

    def __init__(self, state_dim, action_dim, max_size=int(1e6)):
        self.max_size = max_size
        self.ptr = 0
        self.size = 0
        self.state = np.zeros((max_size, state_dim))
        self.action = np.zeros((max_size, action_dim))
        self.next_state = np.zeros((max_size, state_dim))
        self.reward = np.zeros((max_size, 1))
        self.not_done = np.zeros((max_size, 1))
        self.device = DEVICE

    def add(self, state, action, next_state, reward, done):
        self.state[self.ptr] = state
        self.action[self.ptr] = action
        self.next_state[self.ptr] = next_state
        self.reward[self.ptr] = reward
        self.not_done[self.ptr] = 1. - done
        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)

    def sample(self, batch_size):
        ind = np.random.randint(0, self.size, size=batch_size)
        return (
            torch.FloatTensor(self.state[ind]).to(self.device),
            torch.FloatTensor(self.action[ind]).to(self.device),
            torch.FloatTensor(self.next_state[ind]).to(self.device),
            torch.FloatTensor(self.reward[ind]).to(self.device),
            torch.FloatTensor(self.not_done[ind]).to(self.device)
        )


# [修改] 2. 替换数据加载器
def load_neorl2_dataset(replay_buffer: ReplayBuffer):
    """
    从 NeoRL-2 "Pipeline" 环境加载数据集到 ReplayBuffer。
    """
    print("Loading NeoRL-2 'Pipeline' dataset...")
    try:
        # 1. 创建环境以获取数据集
        env = gym.make("Pipeline")
        # NeoRL-2/D4RL数据集中包含 'done' (终止) 和 'truncated' (截断)
        train_data, _ = env.get_dataset()
        env.close()

        # 2. 使用正确的键名
        obs = train_data['obs']
        actions = train_data['action']
        next_obs = train_data['next_obs']
        # NeoRL-2使用 'reward'
        rewards = train_data['reward']

        # 3. 修正：使用 np.logical_or 或 | 进行按元素逻辑或操作
        terminals = np.logical_or(train_data['done'], train_data['truncated'])

        num_transitions = len(obs)

        for i in range(num_transitions):
            s_t = obs[i]
            a_t = actions[i]
            s_t_plus_1 = next_obs[i]
            r_t = rewards[i]
            done = terminals[i]

            # 确保 r_t 是 (1,) 以匹配 buffer 形状
            r_t_reshaped = np.array([r_t]).reshape(1, )

            replay_buffer.add(
                s_t,
                a_t,
                s_t_plus_1,
                r_t_reshaped,
                done
            )

        print(f"✅ Loaded {num_transitions} transitions into real buffer (STATE_DIM={STATE_DIM}).")

    except Exception as e:
        print(f"❌ 错误: 加载 NeoRL-2 数据集失败: {e}")
        print("请确保 'neorl2' 和 'gymnasium' 已正确安装。")
        raise e


# --- 2. 动态模型 (Ensemble MLPs) ---
class DynamicsModel(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=HIDDEN_DIM):
        super(DynamicsModel, self).__init__()
        # 输出维度是 (state_dim + 1) * 2:
        # (delta_s_mean, delta_s_log_std, r_mean, r_log_std)
        output_dim = (state_dim + 1) * 2
        self.mlp = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        self.max_log_std = nn.Parameter(torch.ones(state_dim + 1) * 0.5, requires_grad=False)
        self.min_log_std = nn.Parameter(torch.ones(state_dim + 1) * -10.0, requires_grad=False)

    def forward(self, state, action):
        sa = torch.cat([state, action], dim=-1)
        output = self.mlp(sa)

        # 拆分维度列表: [delta_s_mean, delta_s_log_std, r_mean, r_log_std]
        sizes = [STATE_DIM, STATE_DIM, 1, 1]

        assert sum(sizes) == output.size(-1), "Output dimension mismatch for split"

        delta_s_mean, delta_s_log_std, r_mean, r_log_std = torch.split(output, sizes, dim=-1)

        mean = torch.cat([delta_s_mean, r_mean], dim=-1)
        log_std = torch.cat([delta_s_log_std, r_log_std], dim=-1)

        log_std = torch.clamp(log_std, self.min_log_std, self.max_log_std)
        return mean, log_std


class EnsembleDynamicsModel:
    def __init__(self, state_dim, action_dim, normalizer: Normalizer, num_ensemble=NUM_ENSEMBLE, hidden_dim=HIDDEN_DIM):
        self.num_ensemble = num_ensemble
        self.models = [DynamicsModel(state_dim, action_dim, hidden_dim).to(DEVICE) for _ in range(num_ensemble)]
        self.optimizers = [optim.Adam(model.parameters(), lr=LR_DYN, weight_decay=1e-4) for model in self.models]
        self.state_dim = state_dim
        self.normalizer = normalizer
        print(f"Initialized EnsembleDynamicsModel with {num_ensemble} models.")

    def train(self, real_buffer: ReplayBuffer, epochs=DYNAMICS_TRAIN_EPOCHS, batch_size=BATCH_SIZE):
        for epoch in range(epochs):
            for model_idx in range(self.num_ensemble):
                model = self.models[model_idx]
                optimizer = self.optimizers[model_idx]
                s, a, s_next, r, _ = real_buffer.sample(batch_size)

                # --- 对状态进行归一化 ---
                s_norm = self.normalizer.transform(s)
                s_next_norm = self.normalizer.transform(s_next)
                # 目标是归一化空间中的状态变化量
                delta_s_norm = s_next_norm - s_norm
                target = torch.cat([delta_s_norm, r], dim=-1)

                mean, log_std = model(s_norm, a)
                std = torch.exp(log_std)
                nll_loss = F.gaussian_nll_loss(mean, target, std, reduction='mean')

                optimizer.zero_grad()
                nll_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=40.0)
                optimizer.step()

            if (epoch + 1) % 10 == 0:
                print(f"[Dynamics Training] Epoch {epoch + 1}/{epochs}, NLL Loss: {nll_loss.item():.4f}")

    def predict(self, s, a) -> Tuple[torch.Tensor, torch.Tensor]:
        means, log_stds = [], []
        # --- 预测时也使用归一化的状态 ---
        s_norm = self.normalizer.transform(s)
        for model in self.models:
            mean, log_std = model(s_norm, a)
            means.append(mean)
            log_stds.append(log_std)
        return torch.stack(means), torch.stack(log_stds)

    def step(self, s, a):
        with torch.no_grad():
            means, log_stds = self.predict(s, a)
            stds = torch.exp(log_stds)

            # 从随机选择的模型中采样
            samples = means + torch.randn_like(means) * stds

            model_indices = torch.randint(0, self.num_ensemble, (s.size(0),), device=DEVICE)
            batch_indices = torch.arange(0, s.size(0), device=DEVICE)
            # 选取对应模型的预测样本
            sample = samples[model_indices, batch_indices]

            delta_s_norm, r = torch.split(sample, self.state_dim, dim=-1)

            # --- 将归一化的状态变化量加到归一化的当前状态上 ---
            s_norm = self.normalizer.transform(s)
            next_s_norm = s_norm + delta_s_norm
            # 然后反归一化得到真实尺度的下一状态
            next_s = self.normalizer.inverse_transform(next_s_norm)

            # --- 不确定性惩罚 (基于归一化空间) ---
            # 计算 delta_s_norm 的均值在 Ensembles 上的标准差 (不确定性)
            delta_s_norm_means, _ = torch.split(means, self.state_dim, dim=-1)
            uncertainty = torch.norm(delta_s_norm_means.std(dim=0), dim=-1, keepdim=True)

            reward_means = means[:, :, self.state_dim:]
            mean_r = reward_means.mean(dim=0)
            penalized_r = mean_r - UNCERTAINTY_LAMBDA * uncertainty

            # [修改] 增加 NaN/Inf 检查
            if torch.isnan(next_s).any() or torch.isinf(next_s).any():
                print("\n!!! 严重警告: 动态模型预测的 next_s 中发现 NaN/Inf。正在替换为当前状态 s !!!\n")
                invalid_rows = torch.logical_or(torch.isnan(next_s).any(dim=1), torch.isinf(next_s).any(dim=1))
                next_s[invalid_rows] = s[invalid_rows]
                # 给予一个大的负奖励
                penalized_r[invalid_rows] = -10.0

            return next_s, penalized_r


# --- 3. 策略网络 (SAC) ---
class TanhNormal(TransformedDistribution):
    def __init__(self, base_distribution, transforms):
        super().__init__(base_distribution, transforms)

    @property
    def mean(self):
        # TanhTransform is the first transform
        return self.transforms[0](self.base_dist.mean)


class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=HIDDEN_DIM, max_action=1.0):
        super(Actor, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        self.mean_layer = nn.Linear(hidden_dim, action_dim)
        self.log_std_layer = nn.Linear(hidden_dim, action_dim)
        self.max_action = max_action

    def forward(self, state):
        x = self.mlp(state)
        mean = self.mean_layer(x)
        log_std = self.log_std_layer(x)
        log_std = torch.clamp(log_std, min=-20.0, max=2.0)
        std = torch.exp(log_std)
        base_dist = Normal(mean, std)
        # 使用 cache_size=1 或 0 来避免一个 PyTorch DeprecationWarning
        dist = TanhNormal(base_dist, [TanhTransform(cache_size=1)])
        action = dist.rsample()
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        # 将 Tanh(-1, 1) 的输出缩放到 max_action
        action = action * self.max_action
        return action, log_prob


class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=HIDDEN_DIM):
        super(Critic, self).__init__()
        self.q1_mlp = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        self.q2_mlp = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, state, action):
        sa = torch.cat([state, action], dim=-1)
        q1 = self.q1_mlp(sa)
        q2 = self.q2_mlp(sa)
        return q1, q2


class SAC:
    def __init__(self, state_dim, action_dim, normalizer: Normalizer, max_action=1.0, hidden_dim=HIDDEN_DIM):
        self.actor = Actor(state_dim, action_dim, hidden_dim, max_action).to(DEVICE)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_A)

        self.critic = Critic(state_dim, action_dim, hidden_dim).to(DEVICE)
        self.critic_target = copy.deepcopy(self.critic)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_C)

        self.log_alpha = torch.tensor(np.log(ALPHA), dtype=torch.float32, requires_grad=True, device=DEVICE)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=LR_A)
        self.target_entropy = TARGET_ENTROPY

        self.normalizer = normalizer
        self.total_it = 0

    @property
    def alpha(self):
        return self.log_alpha.exp()

    def select_action(self, state, deterministic=False):
        # 1. 安全地将输入 state 转换为 NumPy 数组 (如果它是 CUDA Tensor，则先 .cpu())
        if isinstance(state, torch.Tensor):
            state_np = state.cpu().numpy()
        else:
            state_np = np.array(state)

        # 2. 追踪原始输入的维度
        original_ndim_is_1 = (state_np.ndim == 1)

        if original_ndim_is_1:
            state_np = state_np.reshape(1, -1)

        # --- 动作选择时也要归一化 ---
        state_norm = self.normalizer.transform(state_np)
        state_tensor = torch.FloatTensor(state_norm).to(DEVICE)

        with torch.no_grad():
            x = self.actor.mlp(state_tensor)

            # 修复：在 if/else 外部计算 mean，因为两个分支都需要它
            mean = self.actor.mean_layer(x) # <--- 修复在此处

            if deterministic:
                # 确定性模式下，使用 Tanh(mean)
                action = torch.tanh(mean) * self.actor.max_action
            else:
                # 随机模式下，使用 rsample()
                log_std = self.actor.log_std_layer(x)
                log_std = torch.clamp(log_std, min=-20.0, max=2.0)
                std = torch.exp(log_std)
                # 现在 mean 已经定义
                base_dist = Normal(mean, std)
                dist = TanhNormal(base_dist, [TanhTransform(cache_size=1)])
                action = dist.rsample() * self.actor.max_action

        # 3. 确保返回的 action 维度正确
        action_np = action.cpu().data.numpy()

        # 4. 根据原始输入维度格式化输出
        if original_ndim_is_1:
            # 如果输入是 (52,)，返回 (1,)
            return action_np.flatten()
        else:
            # 如果输入是 (B, 52)，返回 (B, 1)
            return action_np

    def train(self, batch: Tuple):
        self.total_it += 1
        s, a, s_next, r, not_done = batch

        # --- 训练时归一化状态 ---
        s_norm = self.normalizer.transform(s)
        s_next_norm = self.normalizer.transform(s_next)

        # --- Critic 目标 ---
        with torch.no_grad():
            a_next, log_prob_next = self.actor(s_next_norm)
            q1_targ, q2_targ = self.critic_target(s_next_norm, a_next)
            q_targ = torch.min(q1_targ, q2_targ)
            y = r + not_done * GAMMA * (q_targ - self.alpha * log_prob_next)

        # --- Critic 损失 ---
        q1_curr, q2_curr = self.critic(s_norm, a)
        critic_loss = F.mse_loss(q1_curr, y) + F.mse_loss(q2_curr, y)
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # --- Actor & Alpha 损失 (延迟更新) ---
        if self.total_it % POLICY_UPDATE_FREQ == 0:
            a_new, log_prob_new = self.actor(s_norm)
            q1_new, q2_new = self.critic(s_norm, a_new)
            q_new = torch.min(q1_new, q2_new)
            actor_loss = (self.alpha * log_prob_new - q_new).mean()
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            alpha_loss = -(self.log_alpha * (log_prob_new + self.target_entropy).detach()).mean()
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()

            self.soft_update_target()

    def soft_update_target(self):
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)


# --- 4. MOPO 主算法框架 ---
class MOPO:
    def __init__(self, state_dim, action_dim):
        self.real_buffer = ReplayBuffer(state_dim, action_dim)

        # --- 步骤 0: 加载真实数据 ---
        load_neorl2_dataset(self.real_buffer)

        # --- 步骤 0.5: 初始化 Normalizer 并拟合数据 ---
        self.state_normalizer = Normalizer(state_dim)
        # 仅使用已加载的真实数据来拟合 Normalizer
        self.state_normalizer.fit(self.real_buffer.state[:self.real_buffer.size])

        self.virtual_buffer = ReplayBuffer(state_dim, action_dim, max_size=int(ROLLOUT_LENGTH * 1e5))

        # --- 将 normalizer 传递给动态模型和策略 ---
        self.dynamics = EnsembleDynamicsModel(state_dim, action_dim, self.state_normalizer, num_ensemble=NUM_ENSEMBLE)
        self.policy = SAC(state_dim, action_dim, self.state_normalizer)

        self.rollout_length = ROLLOUT_LENGTH

    def train_dynamics(self):
        print("--- 步骤 1: 开始训练动态模型 ---")
        self.dynamics.train(self.real_buffer, epochs=DYNAMICS_TRAIN_EPOCHS)
        print("--- 动态模型训练完毕 ---")

    def generate_virtual_data(self):
        print("--- 步骤 2: 开始生成虚拟数据 ---")
        # 清空虚拟 buffer
        self.virtual_buffer = ReplayBuffer(STATE_DIM, ACTION_DIM, max_size=self.virtual_buffer.max_size)

        num_rollouts = self.virtual_buffer.max_size // self.rollout_length
        # 从真实数据中采样起始状态 (CUDA Tensor)
        start_states, _, _, _, _ = self.real_buffer.sample(num_rollouts)
        states = start_states

        for _ in range(self.rollout_length):
            # policy.select_action 接收 CUDA Tensor, 返回 NumPy 数组 actions
            actions = self.policy.select_action(states, deterministic=False)
            actions_tensor = torch.FloatTensor(actions).to(DEVICE)

            # dynamics.step 接收 CUDA Tensor (states), 返回 CUDA Tensor (next_states, penalized_rewards)
            next_states, penalized_rewards = self.dynamics.step(states, actions_tensor)

            for i in range(states.size(0)):
                # 将 CUDA Tensors 移到 CPU 并转为 NumPy 存入 Buffer
                self.virtual_buffer.add(
                    states[i].cpu().numpy(),
                    actions[i],
                    next_states[i].cpu().numpy(),
                    penalized_rewards[i].item(),
                    False
                )
            states = next_states
        print(f"--- 虚拟数据生成完毕，大小: {self.virtual_buffer.size} ---")

    def train_policy(self, iterations):
        print("--- 步骤 3: 开始训练策略网络 (SAC) ---")
        for it in range(iterations):
            real_batch = self.real_buffer.sample(BATCH_SIZE // 2)
            virtual_batch = self.virtual_buffer.sample(BATCH_SIZE // 2)
            combined_batch = tuple(
                torch.cat([real, virtual], dim=0)
                for real, virtual in zip(real_batch, virtual_batch)
            )
            self.policy.train(combined_batch)

            if (it + 1) % (iterations // 10 if iterations >= 10 else 1) == 0:
                print(f"[Policy Training] Iteration {it + 1}/{iterations}")
        print("--- 策略网络训练完毕 ---")

    def evaluate_policy(self, eval_episodes=10):
        """
        使用 Neorl2 "Pipeline" 环境在线评估当前策略。
        """
        print(f"--- 步骤 4: 评估策略 (运行 {eval_episodes} 轮) ---")
        try:
            eval_env = gym.make("Pipeline")
        except Exception as e:
            print(f"❌ 评估错误: 无法创建 'Pipeline' 环境: {e}")
            return

        total_rewards = []
        for i in range(eval_episodes):
            obs, info = eval_env.reset()
            state = obs
            episode_reward = 0
            terminated = False
            truncated = False

            while not (terminated or truncated):
                # 策略接收 NumPy 状态，返回 NumPy 动作
                action = self.policy.select_action(state, deterministic=True)
                next_obs, reward, terminated, truncated, _ = eval_env.step(action)
                episode_reward += reward
                state = next_obs

            total_rewards.append(episode_reward)
            print(f"  Eval Episode {i + 1}/{eval_episodes}, Reward: {episode_reward:.2f}")

        eval_env.close()

        avg_reward = np.mean(total_rewards)
        std_reward = np.std(total_rewards)
        print("--------------------------------------------------")
        print(f"✅ 评估完成: 平均累积奖励: {avg_reward:.2f} +/- {std_reward:.2f} (在 {eval_episodes} 轮中)")
        print("--------------------------------------------------")

    def run(self, num_epochs=100, policy_train_iterations=1000, eval_freq=1):
        """
        MOPO 主训练循环。
        """
        # 1. 首先训练动态模型
        self.train_dynamics()

        # 在训练开始前评估一次
        print("\n--- 评估预训练 (Pre-trained) 策略 ---")
        self.evaluate_policy(eval_episodes=5)

        # 2. MOPO 主循环
        for epoch in range(num_epochs):
            print(f"\n--- MOPO 主循环: Epoch {epoch + 1}/{num_epochs} ---")

            # 步骤 2: 生成虚拟数据
            self.generate_virtual_data()

            # 步骤 3: 训练策略
            self.train_policy(iterations=policy_train_iterations)

            # 步骤 4: 定期评估
            if (epoch + 1) % eval_freq == 0:
                self.evaluate_policy()

        print("\n--- 训练结束, 保存模型 ---")
        self.save(base_path="mopo_pipeline_model")

    # --- MOPO 模型保存方法 ---
    def save(self, base_path: str = "agent/mopo_model"):
        import os

        # 创建目录 (如果不存在)
        save_dir = os.path.dirname(base_path)
        if save_dir and not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)

        # 策略模型 (Actor 和 Critic)
        policy_path = f"{base_path}_policy_actor.pth"
        torch.save(self.policy.actor.state_dict(), policy_path)
        print(f"✅ Saved Actor to {policy_path}")

        policy_critic_path = f"{base_path}_policy_critic.pth"
        torch.save(self.policy.critic.state_dict(), policy_critic_path)
        print(f"✅ Saved Critic to {policy_critic_path}")

        # 动态模型 (Ensemble)
        dynamics_dir = f"{base_path}_dynamics_ensemble"
        os.makedirs(dynamics_dir, exist_ok=True)
        for i, model in enumerate(self.dynamics.models):
            model_path = os.path.join(dynamics_dir, f"model_{i}.pth")
            torch.save(model.state_dict(), model_path)
        print(f"✅ Saved {self.dynamics.num_ensemble} Dynamics Models to {dynamics_dir}/")

        # 归一化器 (非常重要，用于部署)
        normalizer_path = f"{base_path}_normalizer.npz"
        np.savez(normalizer_path, mean=self.state_normalizer.mean, std=self.state_normalizer.std)
        print(f"✅ Saved Normalizer to {normalizer_path}")


# --- 5. 执行 ---
if __name__ == "__main__":
    print("===========================================")
    print("--- 初始化 MOPO Agent for Pipeline ---")
    print(f"--- 目标设备: {DEVICE} ---")
    print("===========================================")

    mopo_agent = MOPO(
        state_dim=STATE_DIM,
        action_dim=ACTION_DIM
    )

    print("\n--- 开始 MOPO 训练 ---")

    # [修改] 减少 Epochs 和 Iterations 以便快速测试
    mopo_agent.run(
        num_epochs=20,  # 总轮数
        policy_train_iterations=500,  # 每轮的策略训练步数
        eval_freq=1  # 每轮都评估一次
    )

    print("\n--- MOPO for Pipeline 训练运行完毕 ---")